# **The Ethnographer's Digital Colleague**

This repository contains the full code, data, and model artifacts for the research project, "The Ethnographerâ€™s Digital Colleague: Prototyping a Reflexive AI Partner for Fieldwork Analysis," authored by GÃ¼rhan CamgÃ¶z.

This project details the creation of a fine-tuned Large Language Model (LLM) designed to act as a "digital colleague" for anthropologists and ethnographers. The model is trained to engage in methodologically-aware dialogue, prompting researchers to think more deeply about their observations, analytical frameworks, and ethical considerations during the writing process.

For a full discussion of the project's theoretical background, methodology, and implications, please refer to the complete article.

## **Repository Structure**

This repository is organized to provide a clear and transparent view of the entire research pipeline, from source text processing to model fine-tuning.

/  
â”‚  
â”œâ”€â”€ ðŸ“œ README.md             \# This overview file  
â”‚  
â”œâ”€â”€ ðŸ“‚ data/                  \# The final, processed datasets  
â”‚   â”œâ”€â”€ output\_dataset.jsonl  \# The full 511-example dataset  
â”‚   â”œâ”€â”€ train.jsonl           \# The 80% training split  
â”‚   â”œâ”€â”€ validation.jsonl      \# The 10% validation split  
â”‚   â””â”€â”€ test.jsonl            \# The 10% held-out test split  
â”‚  
â”œâ”€â”€ ðŸ“‚ model/                 \# The fine-tuned model adapter  
â”‚   â””â”€â”€ ethno-colleague-final/  \# LoRA adapter files ready for use  
â”‚  
â”œâ”€â”€ ðŸ“‚ notebooks/  
â”‚   â””â”€â”€ Ethno\_Colleague\_LLM.ipynb \# The Jupyter Notebook for fine-tuning  
â”‚  
â”œâ”€â”€ ðŸ“‚ scripts/               \# Python scripts for the data pipeline  
â”‚   â”œâ”€â”€ 01\_split\_thesis\_subsections.py  
â”‚   â”œâ”€â”€ 02\_process\_thesis\_sections.py  
â”‚   â”œâ”€â”€ 03\_process\_single\_chunk\_mistral.py  
â”‚   â”œâ”€â”€ 04\_process\_personal\_data.py  
â”‚   â”œâ”€â”€ 05\_split\_dataset.py  
â”‚   â””â”€â”€ 06\_analyze\_dataset.py  
â”‚  
â””â”€â”€ ðŸ“‚ source\_texts/          \# The raw .txt files used for generation  
    â”œâ”€â”€ chapter\_chunks/       \# Chunks from the Anthropological Reader  
    â””â”€â”€ thesis\_subsections/   \# Chunks from the author's Master's thesis

## **Setup and Installation**

The project was developed in a Python environment. To replicate the work, you will need the main dependencies, which can be installed via pip.

pip install \-q transformers peft bitsandbytes accelerate datasets trl mistralai pypdf2

A complete list of installations can be found in the Jupyter Notebook (/notebooks/Ethno\_Colleague\_LLM.ipynb).

You will also need a Mistral AI API key, set as an environment variable (MISTRAL\_API\_KEY), to run the data generation scripts.

## **How to Replicate the Project**

The project can be replicated in two main stages: curating the dataset and fine-tuning the model.

### **1\. The Data Curation Pipeline**

The output\_dataset.jsonl file was created by running the scripts in the /scripts/ directory in numerical order. This process involves:

1. **Splitting Source Texts:** Scripts like 01\_split\_thesis\_subsections.py ingest raw PDF files and segment them into thematically coherent .txt chunks, which are saved in the /source\_texts/ subdirectories.  
2. **Generating Data:** Scripts 02 through 04 take these text chunks and use the Mistral Large API to generate the structured instruction/context/output examples. These are appended to a master file.  
3. **Analyzing and Partitioning:** The final scripts, 05\_split\_dataset.py and 06\_analyze\_dataset.py, shuffle the master dataset, analyze its contents, and split it into the final train.jsonl, validation.jsonl, and test.jsonl files found in the /data/ directory.

### **2\. Fine-Tuning the Model**

The model fine-tuning process is documented in the Jupyter Notebook:  
/notebooks/Ethno\_Colleague\_LLM.ipynb  
This notebook provides a step-by-step guide to:

* Loading the base mistralai/Mistral-7B-Instruct-v0.3 model.  
* Configuring the QLoRA parameters for efficient training.  
* Loading the train.jsonl and validation.jsonl datasets.  
* Running the SFTTrainer to fine-tune the model.  
* Saving the final model adapter, which is provided in the /model/ directory.

## **License**

This project, including its code, data, and model artifacts, is released under the Apache 2.0 License. This permissive license was chosen for its compatibility with the license of the base model (mistralai/Mistral-7B-Instruct-v0.3) and to encourage the widest possible use, sharing, and adaptation within the academic and research communities.

## **Ethical Considerations and Data Privacy**

This project was trained on a combination of canonical academic texts and the author's own personal academic writings, including a Master's thesis, field notes, and diaries.

**A note on privacy:** The source PDF files for personal diaries and raw, sensitive field notes **are not included** in this public repository. This is to protect the privacy of the author and the confidentiality of any research participants mentioned therein.

The shared dataset (output\_dataset.jsonl) contains only the AI-generated training examples derived from these texts. While the contexts in the dataset are based on real experiences, they have been processed and reframed by the generator LLM, creating a layer of abstraction from the original source material. No raw, unpublished field notes are present in the dataset.
